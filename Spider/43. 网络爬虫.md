# **Python高级语法——网络爬虫——学习心得笔记**
  
# 1. 网络爬虫
- 爬虫定义：
    网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），
    是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。
    另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。
- 两大特征
    - 按照作者的要求下载数据和内容
    - 能自动在网络上流窜
- 三大步骤
    - 下载网页
    - 提取正确的信息
    - 根据一定的规则自动跳到另外的网页执行以上两步内容
- 爬虫分类
    - 通用爬虫（搜索引擎，类似百度，搜狗）
    - 专用爬虫（聚焦爬虫）
- Python网络爬虫包介绍
    - Python3.x
        - urllib, urllib3, httplib2, requests
        - 常用组合：urllib, requests
        
# 2. urllib包
## 2.1. 包含模块
- urllib.request: 打开和读取urls
- urllib.error: 包含urllib.request产生的常见错误，使用try捕捉                                  
- urllib.parse: 包含即系url的方法
- urllib.robotparse: 解析robots.txt文件
- 查看实例43_1
    
## 2.2. 网页编码问题的解决
- chardet 可以自动检测页面文件的编码格式，但是可能有误 
- 比如上面实例运行后结果开头里面就有：
<head>
    <meta ...... charset=UTF-8"/>    
- 需要安装chardet包
    - pycharm虚拟环境下安装包，直接进入解释器设置界面，点击包列表右边
    - 的加号，进去搜索所需的包，安装即可
    - 自动检测编码看实例
    - 43_2
	    
## 2.3. urlopen的返回对象
- rsp = request.urlopen(url)  
- 有时候不一定能获得对象，断网了，服务器故障等等
- geturl: 返回请求对象的URL
- info: 返回反馈对象的meta信息
- getcode: 返回的http code（状态码）
- 看实例43_3
    
## 2.4. request.data
- 访问网络的两种方法
    - get 
        - 利用参数给服务器传递信息
        - 参数为dict,使用parse编码
        - 看实例43_4
    - post
        - 一般向服务器传递参数使用
        - post是把信息自动加密处理
        - 我们如果想使用post信息，需要使用data参数
        - 使用post,意味着http请求头可能需要更改:
            - Content-Type: application/x-www.form-urlencode
            - Content-Length: 数据长度
            - 简而言之，一旦更改请求方法，注意其它请求头部信息相适应         
            - 看实例43_5 
            - 为了更多的设置请求信息，单纯通过urlopen就不太适用了
            - 需要使用request.Request类     
   