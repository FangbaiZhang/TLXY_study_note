# **Python实战——网络爬虫——页面解析数据提取——学习心得笔记**
  
# 1. 页面解析和数据提取简介
- 结构数据：先有的结构，再谈数据
    - JSON文件
        - JSON Path
        - 转换成Python类型进行操作（json类）
    - XML文件
        - 转化为Python类型进行操作（xml to dict)
        - XPath
        - CSS选择器
        - 正则表达式

- 非结构化数据：现有数据，再谈结构
    - 文本
    - 电话号码
    - 邮箱地址
    - 通常处理上述数据，都有一定的规律，使用正则表达式
    - HTML文件
        - 正则
        - XPath
        - CSS选择器
        
# 2. 正则表达式
- 一套规则，可以字符串文本中进行搜索替换等  
- 案例67_1，re的基本使用流程   
- 正则常用方法函数：

    - compile 函数用于编译正则表达式
    - 生成一个正则表达式（ Pattern ）对象
    - 供 match() 和 search() 这两个函数使用
    - re.match函数
        尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。
        re.match(pattern, string, flags=0)
    - re.search 扫描整个字符串并返回第一个成功的匹配
        re.search(pattern, string, flags=0)
    - group() 或 groups() 匹配对象函数来获取匹配表达式
    - re.sub用于替换字符串中的匹配项
        re.sub(pattern, repl, string, count=0)
    - start() 返回匹配开始的位置
    - end() 返回匹配结束的位置
    - span() 返回一个元组包含匹配 (开始,结束) 的位置 
    - 参考案例36_1/2/3.py
    
- re.match和re.search的区别  
    - re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；
    - 而re.search匹配整个字符串，直到找到一个匹配。 
    
- 匹配中文
    - 大部分中文内容表示范围是[u4e00-u9fa5]  
    - findall找出所有的匹配
    - 看案例36_4
    
- 贪婪和非贪婪
    - 贪婪：尽可能多的匹配（*）表示
    - 非贪婪：找到符合条件的最小内容即可，（？）表示
    - 正则默认使用贪婪匹配
    - 看案例36_5
    
# 3. XML XPath 请参考高级语言中的内容

# 4. lxml
- Python下的HTML/XML的解析器
- 官方文档：https://lxml.de/
- 参考教程：https://www.cnblogs.com/zhangxinqi/p/9210211.html#_label5

- 解析字符串为一个HTML对象
    - 参考实例67_2  
    
- 读取HTML文件，进行解析
    - 参考实例67_3  
    
- etree和XPath的配合使用
    - 参考案例67_4  
    
- 属性匹配格式
    - 参考案例67_5
    
- 综合案例：抓取TIOBE指数前20名排行开发语言
    - 参考案例67_6    
    
# 5. BeautifulSoup4使用
- BeautifulSoup4
- 官方文档地址：
    https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#id7
- 几个常用提取信息工具的比较
    - 正则：很快，但是正则表达式很难完美写出
    - beautifulsoup4: 慢，使用简单，安装简单，数据较少时推荐使用
    - lxml: 比较快，使用简单，安装一般
- BeautifulSoup4的用法
- 参考案例67_7        

## 5.1. HTML或XML中的四大对象
- 四大对象
    - Tag
    - NavigableString
    - Beautifulsoup
    - Comment

- Tag（标签对象）
    - 对应HTML中的标签
    - 可以通过soup.tag_name
    - tag标签有两个重要的属性
        - name
        - attribute(使用时用.attrs)
            - attrs属性是一个字典，修改删除类似于字典
    - 参考实例67_8  
    
- NavigableString（一般为字符串对象）
    - 查看标签内容中的值    
    - 用法tag_name.string
    - 标签中的内容不能编辑，但是可以替换
    - 参考实例67_9,参考实例67_11 
    
- Beautifulsoup（整个文档对象）
    - 表示一个文档的内容，大部分可以当做一个tag对象
    - Beautifulsoup实例的对象包含一个特殊的属性
    - 对象包含了一个值为 “[document]” 的特殊属性 
    - 使用方法 soup.name
    - 参考实例67_9
    
- Comment（注释对象）
    - 通常情况下Tag , NavigableString , BeautifulSoup 几乎覆盖了
    - html和xml中的所有内容,但是还有一些特殊对象.
    - 容易让人担心的内容是文档的注释部分 
    - Comment对象是一个特殊类型的 NavigableString 对象  
    - 参考实例67_10
    
## 5.2. 遍历文档树

### 5.2.1. 子节点

- 子节点
    -  一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.
    - Beautiful Soup提供了许多操作和遍历子节点的属性.  
    - 操作文档树最简单的方法就是告诉它你想获取的tag的name
    - BS可以补全html文档中缺少的标签
    - 通过点取属性的方式只能获得当前名字的第一个tag
    - 得到所有的<a>标签,或是通过名字得到比一个tag更多的内容的时候
        - soup.find_all('a')
        - find_all返回的是一个列表
    - 参考实例67_12,补全后的内容参考67_12.html
    - 爱丽丝梦游仙境的例子，浏览器中显示的文档
    
        The Dormouse's story
        Once upon a time there were three little sisters; 
        and their names were Elsie, Lacie and Tillie; 
        and they lived at the bottom of a well.
        ...

- 子节点中的.contents .children .descendants
    -  .contents 属性可以将tag的子节点以列表的方式输出
    - <html>标签也是 BeautifulSoup 对象的子节点
    - 字符串没有 .contents 属性,因为字符串没有子节点
    - 通过tag的 .children 生成器,可以对tag的子节点进行循环  
    - 但是<title>标签也包含一个子节点:字符串 “The Dormouse’s story”,
    - 这种情况下字符串 “The Dormouse’s story”也属于<head>标签的子孙节点.
    - .descendants 属性可以对所有tag的子孙节点进行递归循环 
    - 参考实例67_13
    
-  .string  .strings 和 stripped_strings
    - 如果tag只有一个 NavigableString 类型子节点,
    - 那么这个tag可以使用 .string 得到子节点
    - 如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,
    - 输出结果与当前唯一子节点的 .string 结果相同
    - 如果tag包含了多个子节点,tag就无法确定
    - .string 方法应该调用哪个子节点的内容,的输出结果是 None 
    - 参考案例67_14
    - 如果包含多个字符串,可以使用 .strings 来循环获取
    - 输出的字符串中可能包含了很多空格或空行,
    - 使用 .stripped_strings 可以去除多余空白内容
    - 参考案例67_15
  
### 5.2.2. 父节点，兄弟节点
- 通过 .parent 属性来获取某个元素的父节点.
    - 在例子“爱丽丝”的文档中,<head>标签是<title>标签的父节点
    - 文档title的字符串也有父节点:<title>标签
    - 文档的顶层节点比如<html>的父节点是 BeautifulSoup 对象
    - BeautifulSoup 对象的 .parent 是None
    - 参考实例67_16
    
- 通过元素的 .parents 属性可以递归得到元素的所有父辈节点   
    - 通过元素的 .parents 属性可以递归得到元素的所有父辈节点,
    - 使用了 .parents 方法遍历了<a>标签到根节点的所有节点
    - .prettify()可以查看兄弟节点
    - 参考实例67_17
    
## 5.3. 搜索文档树 

## 5.3.1. 过滤器 

- find_all()
- find_all() 方法将返回文档中符合条件的所有tag
- 过滤器
    - 过滤器可以被用在tag的name中,节点的属性中,字符串中或他们的混合中   
    - 传入字符串
        - soup.find_all('b')
    - 传入正则表达式
        - 找出所有以b开头的标签
        - soup.find_all(re.compile(r"^b"))
        - 找出所有名字中包含”t”的标签
        - soup.find_all(re.compile(r"t"))
    - 参考实例67_18
    
    - 传入列表参数,
    - Beautiful Soup会将与列表中任一元素匹配的内容返回.
    - 返回值也是一个列表
    - soup.find_all(["a", "b"])
    
    - 传入True,True 可以匹配任何值,
    - 下面代码查找到所有的tag,但是不会返回字符串节点
    - oup.find_all(True)
    - 参考实例67_19
    
- 方法
    - 没有合适的过滤器，可以定义一个方法   
    - 方法只接受一个元素参数,如果这个方法返回 True 
    - 表示当前元素匹配并且被找到,如果不是则反回 False
    - 参考实例67_20

## 5.3.2. find_all()的参数使用

- find_all()的参数使用
- find_all( name , attrs , recursive , string , **kwargs )   
    - name 参数
    - 可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉
    
    - keyword 参数
    - 如果一个指定名字的参数不是搜索内置的参数名,
    - 搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 id 的参数,
    - Beautiful Soup会搜索每个tag的”id”属性
    
    - 有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性
    - 但是可以通过 find_all() 方法的 attrs 参数
    - 定义一个字典参数来搜索包含特殊属性的tag
    
    - 参考实例67_21
    
## 5.3.3. CSS搜索
- 标识CSS类名的关键字 class 在Python中是保留字,
    - 使用 class 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,
    - 可以通过 class_ 参数搜索有指定CSS类名的tag
- class_ 参数
    - 同样接受不同类型的 过滤器 ,字符串,正则表达式,方法或 True  
    - 参考实例67_22
- string 参数
    - 搜文档中的字符串内容.与 name 参数的可选值一样
    - soup.find_all(string="Elsie")
    - 和tag标签混合使用
    - soup.find_all("a", string="Elsie")
    
- 补充
- find_all() 几乎是Beautiful Soup中最常用的搜索方法
- 语法可以简写，以下写法等价
    
        soup.find_all("a")
        soup("a")
        
        soup.title.find_all(string=True)
        soup.title(string=True)
        
## 5.3.4. find()
- find( name , attrs , recursive , string , **kwargs )  
- 比如文档中只有一个<body>标签,那么使用 find_all() 方法
- 来查找<body>标签就不太合适, 
- 使用 find_all 方法并设置 limit=1 参数不如直接使用 find() 方法    
- 下面代码等价

        soup.find_all('title', limit=1)
        [<title>The Dormouse's story</title>]
        
        soup.find('title')
        <title>The Dormouse's story</title>
        
        - 唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,
        - 而 find() 方法直接返回结果     
        
- find_parents( name , attrs , recursive , string , **kwargs )
- find_parent( name , attrs , recursive , string , **kwargs )              
- find_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等.
- find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,   

- find_all_next() 和 find_next()
- find_all_previous() 和 find_previous()

## 5.4. 修改文档树
- 修改方法类似python中的赋值
- 语法如下
    soup = BeautifulSoup('<b class="boldest">Extremely bold</b>')
    tag = soup.b
    
    tag.name = "blockquote"
    tag['class'] = 'verybold'
    tag['id'] = 1
    tag
    # <blockquote class="verybold" id="1">Extremely bold</blockquote>

## 5.5. 格式化输出
- prettify() 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,
- 每个XML/HTML标签都独占一行
- 如下示例
        markup = '<a href="http://example.com/">I linked to <i>example.com</i></a>'
        soup = BeautifulSoup(markup)
        soup.prettify()  
        print(soup.prettify())
        
        # <html>
        #  <head>
        #  </head>
        #  <body>
        #   <a href="http://example.com/">
        #    I linked to
        #    <i>
        #     example.com
        #    </i>
        #   </a>
        #  </body>
        # </html> 
        
       
## 5.6. 解析器的区别
- 主要的解析器,以及它们的优缺点

    Python标准库	BeautifulSoup(markup, "html.parser")	
    Python的内置标准库
    执行速度适中
    文档容错能力强
    Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差

    lxml HTML 解析器	BeautifulSoup(markup, "lxml")	
    速度快
    文档容错能力强
    需要安装C语言库
    lxml XML 解析器	
    BeautifulSoup(markup, ["lxml-xml"])

    BeautifulSoup(markup, "xml")
    速度快
    唯一支持XML的解析器
    需要安装C语言库

    html5lib	BeautifulSoup(markup, "html5lib")	
    最好的容错性
    以浏览器的方式解析文档
    生成HTML5格式的文档
    速度慢
    不依赖外部扩展
    
## 5.7. 编码
- 任何HTML或XML文档都有自己的编码方式,比如ASCII 或 UTF-8,
- 但是使用Beautiful Soup解析后,文档都被转换成了Unicode   
- 编码自动检测_ 功能大部分时候都能猜对编码格式,但有时候也会出错.
- 有时候即使猜测正确,也是在逐个字节的遍历整个文档后才猜对的,这样很慢.
- 如果预先知道文档编码,可以设置编码参数来减少自动检查编码出错的概率
- 并且提高文档解析速度
- 语法
    soup = BeautifulSoup(markup, from_encoding="iso-8859-8")
    
- 通过Beautiful Soup输出文档时,不管输入文档是什么编码方式,
- 输出编码均为UTF-8编码

## 5.8. 补充知识
- 复制Beautiful Soup对象
- copy.copy() 方法可以复制任意 Tag 或 NavigableString 对象
        import copy
        p_copy = copy.copy(soup.p)
        print p_copy
        # <p>I want <b>pizza</b> and more <b>pizza</b>!</p>
    
            
     
        
    

               
