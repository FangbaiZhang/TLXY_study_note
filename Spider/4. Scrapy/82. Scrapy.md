# **Python实战——网络爬虫——Scrapy——学习心得笔记**
  
# 1. Scrapy介绍
- 爬虫框架
    - scrapy 最常用
    - pyspider 一个小框架
    - crawley
    
- scrapy框架介绍
    - 官网:
        - http://www.scrapyd.cn/
    - 官方文档：
        - http://www.scrapyd.cn/doc/
        - https://doc.scrapy.org/en/latest/#
        
- scrapy安装
    - 安装参考
    - Scrapy安装说明及如何避坑
    - https://blog.csdn.net/u011318077/article/details/86661924  
    
- scrapy概述
    - 包含的部件    
        - ScrapyEngine: 神经中枢，大脑，核心
        - Scheduler调度器：引擎发来的request请求，调度器需要处理，然后交换引擎
        - Downloader下载器：把引擎发来的requests请求，得到response
        - Spider爬虫：负责把下载器得到的网页和结果进行分解，分解成数据和连接
        - ItemPipeline管道：详细处理Item
        - DownloaderMiddleware下载中间件：自定义下载功能的扩展组件
        - SpiderMiddleware爬虫中间件：对spider进行扩展
        
- scrapy工作原理
    - 参考：Scrapy工作原理.jpg
    - ScrapyEngine开始-Scheduler-Downloader-Spiders(数据到ItemPipeline,同时进行下一个循环ScrapyEngine)   
        
- 爬虫项目创建流程
    - 新建项目：scrapy startproject xxx(项目文件夹名称)      
        - 打开CMD窗口，CD命令切换到要放置爬虫项目文件夹下面，然后执行上述命令
        - 会自动爬虫项目的文件夹xxx,xxx文件夹里面有一个spiders的文件夹，还有一些py文件
    - 明确需要的目标和产出：编写item.py
    - 制作爬虫：地址 spider/xxspider.py
    - 存储内容：pipelines.py

# 2. Scrapy部件 

# 2.1. ItemPipeline

- ItemPipeline
    - 爬虫提取出数据存入item后，item中保存的数据需要进一步的处理，比如清洗，去重，存储等
    - pipeline需要处理process_item函数
        - process_item:
            - spider提取出的item作为参数传入，同时传入的还有spider
            - 此方法必须实现
            - 必须返回一个item对象，被丢弃之后的item不会被之后的pipeline处理
    - __init__: 构造函数
        - 进行一些必要的参数初始化
    - open_spider(spider):
        - spider对象被开启时调用
    - close_spider(spider):
        - spider对象被关闭时调用  

# 2.2. Spider
    
- Spider
    - 对应的是文件夹spiders下的文件
    - __init__: 初始化爬虫名称，start_urls列表
    - start_requests: 生成Requests对象交给Scrapy下载并返回response
    - parse：根据返回的response解析出相应的item，item自动进入pipeline：如果需要，
    - 解析出url,url自动交给requests模块，一直循环下去
    - start_request: 此方法仅能被调用一次，读取start_urls内容并启动循环过程
    - name；设置爬虫名称
    - start_urls: 设置开始第一批爬取的url
    - allow_domains: spider允许爬取的域名列表
    - log: 日志记录
    
# 2.3. DownloaderMiddleware（中间件）

- 中间件是处于引擎和下载器中间的一层组件
- 可以有很多个，被按循序加载执行
- 作用是对发出的请求和返回的结果进行预处理
- 在Middleware文件中
- 需要在settings中设置以便生效
- 编写中间件必须是scrapy.contrib.downloadermiddleware.DownloaderMiddleware的子类
- 一般一个中间件只完成一个功能
- 必须实现以下一个或者多个方法
    - process_request(self, request, spider)
        - 在request通过的时候被调用
        - 必须返回None或Response或Request或raise IgnoreRequest
    - process_response(self, request, response, spider)
    
# 3. 爬虫去重

- 去重
    - 为了防止爬虫陷入死循环，需要去重
    - 即在spider中的parse函数中，返回request的时候加上dont_filter=False参数
    - 语法
        
        myspider(scrapy.Spider):
            def parse(.......):
                
                .......
                
                yield scrapy.Request(url=url, callback=self.parse, dont_filter=False)
                
- 如何在scrapy使用selenium
    - 模拟浏览器很消耗资源，万不得已不使用
    - 使用可以放在中间件中的process_request函数中
    - 在函数中调用selenium，完成爬取后返回Response
    - 语法
    
        class MyMiddleWare(object):
            def process_request(...):
                
                # 调用selenium
                driver = webdriver.Chrome()
                html = driver.page_source
                driver.quit()
                
                return HtmlResponse(url=request.url, encoding='utf-8', body=html, request=request)

# 4. 简单实例（爬取一条名言）
- 创建一个简单的爬虫实例 
    - 第一步：创建一个scrapy项目
        - 创建命令：scrapy startproject mingyan2(mingyan2是我取的项目文件夹名称)      
        - 打开CMD窗口，CD命令切换到要放置爬虫项目文件夹下面，然后执行上述命令
        - 执行命令及文件夹下面的有哪些东西参考: 爬虫1.png 爬虫2.png     
    - 第二步：编写爬虫的主程序py文件
        - 在spiders文件夹下新建mingyan_spider.py文件
        - 然后里面编写代码
        - 参考mingyan_spider.py文件
    - 第三步：执行爬虫命令
        - CD到刚刚的项目文件mingyan2下
        - 执行命令：scrapy crawl mingyan2
        - 刚刚主程序里面定义了一个，name = 'mingyan2'
        - 上面的命令中的mingyan2是这个name的名称，不是文件夹的名称，只不过这里名称一样
        - 执行结束后，mingyan2文件下自动保存了刚刚爬取到的两个url的页面  
        - 参考: 爬虫3.png  
        
- 参考实例mingyan_spider.py
 
        
# 5. scrapy shell使用
- scrapy调试工具
- CMD窗口切换到项目文件下执行命令：scrapy shell 网址
    - scrapy shell http://lab.scrapyd.cn
    - scrapy shell http://www.baidu.com
- 退出调试执行命令：exit
- 参考: 爬虫4.png  爬虫5.png 

# 6. scrapy css选择器使用(爬取一页的名言)
- 执行调试命令：scrapy shell http://lab.scrapyd.cn
- 先提取HTML的title: response.css('title')
    - 得到一个Selector的列表
    - [<Selector xpath='descendant-or-self::title' data='<title>爬虫实验室 - S
        CRAPY中文网提供</title>'>]
    - 换种写法，每次可以得到不同的结果
    - response.css('title').extract()
        ['<title>爬虫实验室 - SCRAPY中文网提供</title>']
    - response.css('title').extract()[0]
        '<title>爬虫实验室 - SCRAPY中文网提供</title>'
    - response.css('title').extract_first()
        '<title>爬虫实验室 - SCRAPY中文网提供</title>'
    - response.css('title::text').extract_first()
        '爬虫实验室 - SCRAPY中文网提供'
- 我们其实一般都值需要标签里面的具体内容所有执行
    - response.css('title::text').extract_first()
        - 参考：爬虫6.png 
        
# 6.1. scrapy css选择器进一步使用  
- 先项目文件下执行：scrapy shell http://lab.scrapyd.cn  
- 接着执行： mingyan1 = response.css('div.quote')[0]
- 每一段名言都被一个 <div class="quote post">……</div> 包裹
- 我们就把第一段名言保存在：mingyan1  这么一个变量里面了。
- 为什么会有一个：[0] 这表示提取第一段，如果没有这个限制，那我们提取的是本页所有名言
- 网页源码
        <div class="quote post">
	        <span class="text">看官，此页面只为爬虫练习使用，都是残卷，若喜欢可以去找点高清版！</span>
	        <span>作者：<small class="author">中国传世名画</small>
	        <a href="http://lab.scrapyd.cn/archives/57.html">【详情】</a>
	        </span>
	        <p></p>
	        <div class="tags">
	           	标签： <a class='tag' href=http://lab.scrapyd.cn/tag/%E8%89%BA%E6%9C%AF/>艺术</a>，
	           	      <a class='tag' href=http://lab.scrapyd.cn/tag/%E5%90%8D%E7%94%BB/>名画</a>	        
	        </div>
    	</div>	
    	
- 继续执行：mingyan1.css('.text::text').extract_first()
- 使用了：.text这是class选择器，提取第一个内容
- mingyan1.css('.tags .tag::text').extract()
- 我们用的并非是.extract_first()  而是 extract()，why？应为里面有多个标签，我们并非只是提取一个，
- 而是要把所有标签都提取出来，因此就用了：.extract() 
- 并且用了两次标签tags,意思是提取第二个标签里面的内容

- 参考：爬虫7.png
- 参考网址：http://www.scrapyd.cn/doc/147.html
- 参考实例ItemSpider.py
- 参考实例ListSpider.py


# 7. Scrapy项目常用命令
- 先打开CMD命令，CD切换到要创建的项目的文件夹下
    - scrapy startproject（创建项目）
- 创建项目之后切换到项目文件夹下
    - scrapy crawl XX（运行XX蜘蛛）
    - scrapy shell http://www.scrapyd.cn（调试网址为http://www.scrapyd.cn的网站）
- scrapy version 查看版本
- scrapy list
    - 这里的蜘蛛就是指spider文件夹下面xx.py文件中定义的name，
    - 你有10个py文件但是只有一个定义了蜘蛛的name，那只算一个蜘蛛
    - 比如我spider文件夹有3个xxx.py,每个里面都只定义了1个蜘蛛，输出如下
    - D:\Hello World\python_work\TLXY_study_note\Spider\mingyan2>scrapy list
        ItemSpider
        ListSpider
        mingyan2
        
# 8. scrapy分布式爬取(爬取多个页面的多个数据)
- 该网站为例：http://lab.scrapyd.cn
- 查看源代码，找到下一页的连接所在位置
    <li class="next">
    <a href="http://lab.scrapyd.cn/page/2/">下一页 »</a>
    </li>
- 爬取思路：
    - 我们每爬一页就用css选择器来查询，是否存在下一页链接，
    - 存在：则爬取下一页链接：http://lab.scrapyd.cn/page/*/，
    - 然后把下一页链接提交给当前爬取的函数，继续爬取，继续查找下一页，
    - 直到找不到下一页，说明所有页面已经爬完，那结束爬虫。
    - 先提取下一页的连接，并判断是否有下一页连接这个这个值   
         next_page = response.css('li.next a::attr(href)').extract_first()
         if next_page is not None:
    - 然后执行urljoin后我们最终next_page的网址就是http://lab.scrapyd.cn/page/2
    - scrapy给我们提供了这么一个方法：scrapy.Request()
    - 参考实例中的详细注释
              
- 参考实例NextSpider.py    


    
                

    
        
    - 
        
        



        
        
        
        
      
        
        
             
                
                
                   
       
             
              
                 