# **Python实战——网络爬虫——Scrapy——学习心得笔记**
  
# 1. Scrapy介绍
- 爬虫框架
    - scrapy 最常用
    - pyspider 一个小框架
    - crawley
    
- scrapy框架介绍
    - 官网:
        - http://www.scrapyd.cn/
    - 官方文档：
        - http://www.scrapyd.cn/doc/
        - https://doc.scrapy.org/en/latest/#
        
- scrapy安装
    - 安装参考
    - Scrapy安装说明及如何避坑
    - https://blog.csdn.net/u011318077/article/details/86661924  
    
- scrapy概述
    - 包含的部件    
        - ScrapyEngine: 神经中枢，大脑，核心
        - Scheduler调度器：引擎发来的request请求，调度器需要处理，然后交换引擎
        - Downloader下载器：把引擎发来的requests请求，得到response
        - Spider爬虫：负责把下载器得到的网页和结果进行分解，分解成数据和连接
        - ItemPipeline管道：详细处理Item
        - DownloaderMiddleware下载中间件：自定义下载功能的扩展组件
        - SpiderMiddleware爬虫中间件：对spider进行扩展
        
- scrapy工作原理
    - 参考：Scrapy工作原理.jpg
    - ScrapyEngine开始-Scheduler-Downloader-Spiders(数据到ItemPipeline,同时进行下一个循环ScrapyEngine)   
        
- 爬虫项目创建流程
    - 新建项目：scrapy startproject xxx(项目文件夹名称)      
        - 打开CMD窗口，CD命令切换到要放置爬虫项目文件夹下面，然后执行上述命令
        - 会自动爬虫项目的文件夹xxx,xxx文件夹里面有一个spiders的文件夹，还有一些py文件
    - 明确需要的目标和产出：编写item.py
    - 制作爬虫：地址 spider/xxspider.py
    - 存储内容：pipelines.py

# 2. Scrapy部件 

# 2.1. ItemPipeline

- ItemPipeline
    - 爬虫提取出数据存入item后，item中保存的数据需要进一步的处理，比如清洗，去重，存储等
    - pipeline需要处理process_item函数
        - process_item:
            - spider提取出的item作为参数传入，同时传入的还有spider
            - 此方法必须实现
            - 必须返回一个item对象，被丢弃之后的item不会被之后的pipeline处理
    - __init__: 构造函数
        - 进行一些必要的参数初始化
    - open_spider(spider):
        - spider对象被开启时调用
    - close_spider(spider):
        - spider对象被关闭时调用  

# 2.2. Spider
    
- Spider
    - 对应的是文件夹spiders下的文件
    - __init__: 初始化爬虫名称，start_urls列表
    - start_requests: 生成Requests对象交给Scrapy下载并返回response
    - parse：根据返回的response解析出相应的item，item自动进入pipeline：如果需要，
    - 解析出url,url自动交给requests模块，一直循环下去
    - start_request: 此方法仅能被调用一次，读取start_urls内容并启动循环过程
    - name；设置爬虫名称
    - start_urls: 设置开始第一批爬取的url
    - allow_domains: spider允许爬取的域名列表
    - log: 日志记录
    
# 2.3. DownloaderMiddleware（中间件）

- 中间件是处于引擎和下载器中间的一层组件
- 可以有很多个，被按循序加载执行
- 作用是对发出的请求和返回的结果进行预处理
- 在Middleware文件中
- 需要在settings中设置以便生效
- 编写中间件必须是scrapy.contrib.downloadermiddleware.DownloaderMiddleware的子类
- 一般一个中间件只完成一个功能
- 必须实现以下一个或者多个方法
    - process_request(self, request, spider)
        - 在request通过的时候被调用
        - 必须返回None或Response或Request或raise IgnoreRequest
    - process_response(self, request, response, spider)
    
# 3. 爬虫去重

- 去重
    - 为了防止爬虫陷入死循环，需要去重
    - 即在spider中的parse函数中，返回request的时候加上dont_filter=False参数
    - 语法
        
        myspider(scrapy.Spider):
            def parse(.......):
                
                .......
                
                yield scrapy.Request(url=url, callback=self.parse, dont_filter=False)
                
- 如何在scrapy使用selenium
    - 模拟浏览器很消耗资源，万不得已不使用
    - 使用可以放在中间件中的process_request函数中
    - 在函数中调用selenium，完成爬取后返回Response
    - 语法
    
        class MyMiddleWare(object):
            def process_request(...):
                
                # 调用selenium
                driver = webdriver.Chrome()
                html = driver.page_source
                driver.quit()
                
                return HtmlResponse(url=request.url, encoding='utf-8', body=html, request=request)
                
             
                
                
                   
         
             
              
                 